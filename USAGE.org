#+TITLE: Using Brunel's tqZ code

The executable =analysisMain.exe=, contains a number of methods which
are used to produce results for analysis of tqZ. These methods are
accessible through a number of arguments. This documentation covers the
standard processes in using the executable, and its compilation. It is
by no means a complete guide, but one intended provide a starting point
and to give others documentation which the author desperately lacked
during his student days. The author takes no responsibility for any
unintended consequences stemming from use of these instructions.

If it is not covered in this document, your best bet is to run
=./bin/analysisMain.exe --help= to display a help message displaying all the
argument options. If that fails, my only advice is to prepare for some
diving into code to find your answer...

* Compiling the program

In order to compile the program using the provided makefile, the user
needs to execute the following console command in the program's main
directory:

#+BEGIN_SRC sh
    make 
#+END_SRC

The makefile will need to be modified to take into account local library
locations and the user will need root and libconfig++ libraries installed for
the program to compile and execute.

* Configuration Files

When running any part of the analysis process, the executable needs a
configuration file to use. The example configuration file =eeeConf.txt=
is found in the directory =configs/= and the various example sub
configuration files are found in the sub-directories noted in the
example configuration file =eeConf.txt=. The contents are:

#+BEGIN_EXAMPLE
    configuration file "eeConf.txt". The contents are:
    datasets = "configs/datasets/eeData.txt";
    cuts = "configs/cuts/eeCuts.txt";
    plots = "configs/plots/plotConf.cfg";
    outputFolder = "plots/ee/";
    outputPostfix = "ee";
    channelName = "ee";
#+END_EXAMPLE

-  datasets :: the location of the text file denoting each physics
   channel, the location of the root file list for the channel, the
   number of events and cross section for the channel, the data type (MC
   or data), and histogram name, colour, label and plot type.
-  cuts :: the location of the text file denoting the physics cuts for
   tight and loose electrons and muons. This doesn't' need altering for
   new config files unless one has new selection cuts. plots: the
   location of the text file with the instructions for plot names, axis
   widths, number of bins and axis labels.
-  outputFolder :: the output directory for plots.
-  outputPostfix :: the suffix used for output plots.
-  channelName name :: of physics channel being analysed.

The sub-configuration files for datasets follow this general structure:

#+BEGIN_EXAMPLE
    [tZq]
    fileName = configs/datasets/fileLists/tZqFiles.txt
    totalEvents = 1038665
    crossSection = 0.03590
    runType = mc
    histoName = tZq
    colour = kYellow
    label = tZq
    plotType = f
#+END_EXAMPLE

The =fileName= parameter denotes the path to a text file which lists the
locations of the various input files (usually the output of the
nTupiliser). Hopefully the other contents should be self-explanatory.

It is also worth noting that whilst the lepton channel is determined in
the configuration file, there is an argument which overrides this:

-  =-k= : bit mask for lepton channel selection; 1 - ee, 2 - µµ, 4 - ee (same charge), 
   8 - µµ (same charge), 16 -- eµ, 32 - eµ (same charge). To run multiple channels 
   in the same session, add the digits together, eg. 15 -- all lepton channels.

* Creating skims

The first stage of producing results involves the creating of skim
files. Before running this stage, the directory =skims= needs creating.

To create the post-trigger skims (where the singlel and double lepton datasets are combined, with double counting being avoided), one uses the following command:

#+BEGIN_SRC sh
    ./postTriggerSkimmer.exe -c <channel> -d <double-lepton-dir(s)> -s <single-lepton-dir(s)> -o <output-dir-name>
#+END_SRC

The arguments are described as follows:

-  =-c <channel>=: channel being looked over. So if combining single and double MuonEG datasets, this would be <emu>.
-  =-d=: the location of the double lepton input dataset(s) skims from the nTupliser.
-  =-s <bit-mask>=: the location of the single lepton input dataset(s) skims from the nTupliser.
-  =-o <output-dir-name>=: the output directory name. So if combining single and double MuonEG datasets for Run2016C, this would be <emuRun2016C>.

To create the MC and post-trigger skims one uses the following command:

#+BEGIN_SRC sh
    ./analysisMain.exe -c <user-config-file> -g -k <channels> --2016 --dilepton
#+END_SRC

The arguments are described as follows:

-  =-c <user-config-file>=: see above.
-  =-g=: make post-lepton selection trees (including b-tagging efficiency trees) in the skim files.
-  =-k <bit-mask>=: see above (optional).
-  =--2016=: Run in 2016 mode (SFs/corrections for 2016 used), in lieu of the default 2015 mode.
-  =--dilepton=: Run in the dilepton search mode, in leiu of the default to run the trilepton search mode.

* Creating mvaFiles

The second stage of producing results initially involves the creating of
mva files. Before running this stage, the directories =mvaTest=,
=mvaDirs/inputs= and =mvaDirs/skims= need creating. To create the mva
files one uses the following command:

#+BEGIN_SRC sh
    ./analysisMain.exe.exe -c <user-config-file> -k <channels to run over> -u -t -v <SYST> -z
    --dilepton --2016 --mzCut 20. --mwCut 20. --mvaDir <mva skims output directory>
#+END_SRC

The arguments are described as follows:

-  =-c=: :<user-config-file>: see above.
-  =-u=: use the post-lepton selection trees in the mva files.
-  =-v <SYST>=: do the desired systematic. This argument isn't required if one
  is running the program over systematic files, by their virtue of already being
  systematics! SYST is defined as 2^{Number of up and down systematics} - 1. So
  for systematics for trigger, JER, JES, pileup, b-tagging, PDFs, ME and alphaS,  SYST =
  (2^{8\times2} - 1) = 65535. N.B. alphaS weights are currently set to unity, so while
  they are processed for mva skims, they are currently not created for the mva inputs
  (see below).
-  =-z, --makeMVATree=: produce a tree after event selection for mva
   purposes.
-  =-k <bit-mask>=: see above (optional).
-  =-t:= use B-Tagging reweighting.
-  =--jetRegion <nJets,nBjets,maxJets,maxBjets>=: Sets the jet region to
   be looked out (optional).
-  =--mvaDir=: <directoryPath>: custom directory to output mva
   files to (optional).
-  =--metCut=: the cut on the MET one wishes to use during the analysis
   (optional).
-  =--mtwCut=: the cut on the W's transverse mass one wishes to use
   during the analysis (optional).
-  =--2016=: Run in 2016 mode (SFs/corrections for 2016 used), in lieu of the default 2015 mode.
-  =--dilepton=: Run in the dilepton search mode, in leiu of the default to run the trilepton search mode.

* Converting mva skims to mva inputs

After the creation of the mva skims, they need converting to the format used
by the mva tools. Previously this used a python script =scripts/legacy/makeMVAInputDilepton.py=
that was based off the trilepton equivalent =scripts/makeMVAInput.py=. Since then,
the script has been ported in stage to C++ in order to speed up processing samples.
Currently the C++ program (=bin/makeMVAinputMain.exe=) is stable and gives consistent results with the old python
script for data and MC, but is still buggy with respect to the non-prompt-lepton samples.

The executable used to run the C++ code is =./bin/makeMVAinputMain.exe=, with the command arguements:

-  =-h [--help]==: Prints a help message.
-  =-d [--data]==: Runs in data processing mode. If this or NPL mode isn't set, runs in MC mode.
-  =-n [--NPLs]==: Runs in NPL processing mode. If this or data mode isn't set, runs in MC mode.
DO NOT USE EXCEPT FOR DEBUGGING - CURRENTLY BUGGY.
-  =-m [--met]==: Use old MET uncertainity recipie for MCs sample (shouldn't be usually used).
-  =--ttbar=: Runs in ttbar control region mode.
-  =--zPlus=: Runs in the old Z+jets control region mode (i.e. zero b-jets).
-  =-i=: The input directory where the mva skims are read in from.
-  =-o=: The output directory where mva input files are written to.
-  =-s=: Makes signal and sideband regions.

Below are the standard recipes currently used to create mva inputs for data, MC and NPLs.

For example, to create data mva inputs, one uses the following command:
#+BEGIN_SRC sh
    ./bin/makeMVAinputMain.exe -i <mva skims directory> -o <mva inputs output directory> --data
#+END_SRC

Creating MC mva inputs however, is less straightforward due to a number of processes having samples
for different end states. This is an issue as during the mva tools (both ours and Strasbourg's)
reads in samples for a whole process, e.g. WW which is composed of WW2l2nu and WW1l1nu, but if these
samples are created separately then the output TTrees will have different names or if the file is updated,
ROOT only reads the last written tree.
To get around this, for samples such as WW, WZ, ZZ, ttH, ttbarH, etc, need processing individually before being
merged using hadd.
The author's approach is to write the ouputted mva inputs to a separate directory, rename them, and repeat until all
component samples are made and then to "hadd" them and copy them to the intended mva inputs directory.

The list of samples that are not impacted by this merging issue are given in lines 322-327 in =src/common/makeMVAinputAlgo.cpp=
and are uncommented by default. Dedicated theory samples required to evaluate theory systematic uncertainities are listed
in commented out lines (and will need uncommenting and the code rebuilding to run).

Out of the box then, the majority of the MC samples can be simply processed by running the following command:

For example, to create data mva inputs, one uses the following command:
#+BEGIN_SRC sh
    ./bin/makeMVAinputMain.exe -i <mva skims directory> -o <mva inputs output directory>
#+END_SRC

Unfortunately, while the C++ code is stable for creating NPL mva inputs, currently it is considered buggy as
it does not give consistent results with the original python script (unlike the data and MC). Therefore, it is
currently recommended to use a stripped down version of the old python mva input creation script to create the
NPL mva inputs. Thankfully this doesn't take too much time to run (unlike ttbar theory samples in the ttbar CR
with the original python script).

To create the NPL mva inputs one uses the following command:

#+BEGIN_SRC sh
    python ./scripts/runDilepton2016OutputToBDT_Fakes.py <METcut> <mtwCut>
#+END_SRC

This script feeds in the necessary command line arguements to =scripts/makeMVAInputDileptonFakes.py= in order
to create the NPL mva inputs. If one wants to alter the output directory of these mva inputs, one can easily
edit the script =scripts/runDilepton2016OutputToBDT_Fakes.py=.

The arguments are described as follows:

-  =<METcut>=: the cut on the MET one wishes to use during the analysis.
-  =<mtwCut>=: the cut on the W's transverse mass one wishes to use
   during the analysis.
   

* Producing Plots

An optional stage involves the creation of output plots. Before running
this stage, the directory =plots/<channel-name>= (eg. =<channel-name>=
could be =ee=) needs creating. To create the plots one uses the
following command:

#+BEGIN_SRC sh
    ./analysisMain.exe -c <user-config-file> -p
#+END_SRC

The arguments are described as follows:
-  =-p=: makes all plots.
-  =-k=: <bit-mask>=: see above (optional).
- =--NPLs=: for configs with the prefix "prompt" (where "histoName" and "label" in the configs have been set to
specific strings that the code searches for when this flag is enabled) which run over both signal region and 
same charge regions to produce plots with NPLs present.

* Running the BDT

The stage of the analysis uses a slightly altered version of jandrea's
=SingleTop\_tZ\_Macro= [[https://github.com/jandrea/SingleTop_tZ_Macro]],
where =/TMVA/theMVAtool.C= has been altered so that our variables and
input files are used. This slightly altered code can be found here
[[https://github.com/davidcarbonis/SingleTop_tZ_Macro]].

Before using this macro, the input mva files from the previous stage
must be either copied to =/TMVA/inputfiles/= or the macro
=/TMVA/theMVAtool.C= must be amended to reflect your chosen input
directory.

To use the macro, one, in the directory =/TMVA=, uses the following
console commands:

#+BEGIN_SRC c++
    root -l theMVAtool.C+
    theMVAtool tmva
    tmva.doTraining(<inputDir>, <channel>, <numberOfTrees>)
    tmva.doReading(<inputDir>, <outputDir>, <channel>)
#+END_SRC

The training produces the weight files for the BDT, and on completion
loads up the TMVA GUI, which has various options to see how the training
and test samples fare and the performance of the BDT.

The reading reads the TTree and calculates the BDT output for all events
in the evaluation sample and produces templates (MVA distribution).

The arguments are described as follows:

-  =<inputDir>=: Directory containing the input files to be read into
   the TMVA's BDT algorithm for both the training and reading stages.
   Default is =inputroot/met0mtw0/=.
-  =<outputDir>=: Directory containing the input files to be read into
   the TMVA's BDT algorithm for the reading stage. Default is
   =outputroot/met0mtw0/=.
-  =<outputDir>=: Flag for which channel to be run over. Choices are
   =all=, =eee=, =eemu=, =emumu=, and =mumumu=. Default is =all=.
-  =<numberOfTrees>=: The number of trees the BDT is to be trained over.
   Default is =100=. Note that currently the depth of the trees is hard
   coded, a further argument to include this functionality in a more
   flexible and user friendly manner is antipciated in the near future.

* Producing input for theta

Following running the training and reading of the BDT trees in stage 3,
one can create the files necessary to run theta with. The directory
=/TMVA/TemplateRootFiles= must exist. Firstly, the output files from the
previous step need to be merged, through the following console commands
(or for whichever channel you wish to run over):

#+BEGIN_SRC sh
    hadd outputroot/output_merged.root outputroot/.../output_all_*.root
    hadd outputroot/output_merged_eee.root outputroot/.../output_eee_*.root
    hadd outputroot/output_merged_eemu.root outputroot/.../output_eemu_*.root
    hadd outputroot/output_merged_emumu.root outputroot/.../output_emumu_*.root
    hadd outputroot/output_merged_mumumu.root outputroot/.../output_mumumu_*.root
#+END_SRC

Following this, to use the macro which produces the input files for
theta, use the following console command:

#+BEGIN_SRC sh
    root -l ProdTemplate.C+
#+END_SRC

Following this, the output (input for theta) is found in
=/TMVA/TemplateRootFiles=. Note, if you are creating the output for
different cuts, the =ProdTemplate.C= output names won't label the cut
which has been run over.

* Systematics

Currently the analysis code looks after the following systematics:

-  Trigger systematics
-  Jet Energy Resolution (JER) systematics
-  Jet Energy correction Scale factor (JES) systematics
-  Pileup systematics
-  b-tagging systematics
-  Parton-Distribution-Function (PDF) systematics
-  Below is a brief guide as to where they are located and how to up

Below is a brief guide as to where they are located and how to update
them.

Renormalisation and Factorisation Scale Factors are not implemented yet,
but are explained below also.

** Trigger Systematics

For the trigger systematics, the Scale Factors (SF) are applied to MC
datasets only. The relevant code can be found on lines 876-929 in
=src/common/analysisAlgo.cpp=. Currently the SF values for the various
lepton searches are hard-coded, but it is intended to separate them from
the main body of code into a separate function. The values for the SFs
can be found here:

https://twiki.cern.ch/twiki/bin/view/CMS/TopTrigger#Trigger_scale_factors.

The trilepton SFs have not been updated since Run 1 since the Run 2
analyses are focusing on the dilepton final state.

** JER Systematics

For the JER systematics, the Scale Factors (SF) are applied to MC
datasets only. The relevant code can be found in the function
=Cuts::getJetLVec= in =src/common/cutClass.cpp=. Currently the SF values
for the JER SFs are hard-coded (in various eta bins), but it is intended
that these values will be retrieved via CMSSW and stored in nTuples
during the next generation of nTuples. The values for the SFs can be
found here:

[[https://twiki.cern.ch/twiki/bin/viewauth/CMS/JetResolution#MC_truth_JER_at_13_TeV_new]]

Depending on whether the RECO jet is "well matched" to a GEN jet or not (i.e.
dR < R_{cone} / 2), the jet transverse momentum is smeared in one of two ways so
that the pT resolution would be the same as we would measure it in data.

Well matched jets: scaling. Scale corrected p_{T} based on the p_{T} difference
between RECO and GEN jets using:

p_{T} \to \max(0, p_{T}^{GEN} + SF(p_{T}^{RECO} - p_{T}^{GEN}))

Poorly matched jets: smearing. Randomly smear the RECO jet p_{T} using a
Gaussian of width:

\sigma_{SF} √(SF^{2} - 1)

** JES Systematics

For the JES systematics, the Scale Factors (SF) are applied to MC datasets only.
The relevant code can be found in the function =Cuts::getJECUncertainity= in
=src/common/cutClass.cpp=, and it is called in the =Cuts::getJetLVec= function
in the same file. This function applies the Jet Energy Correction Uncertainties,
which are read in from a text file. This text file is loaded by the function
=Cuts::initialiseJECCors= in the same file. In the future, it is intended that
these uncertainties will be retrieved via CMSSW and stored in nTuples during the
next generation of nTuples.

** Pileup Systematics

The pileup model we use has several sources of systematic error: uncertainty in
the number of interactions, systematic shifts in the reweighting process, and
other effects (see
[[https://twiki.cern.ch/twiki/bin/view/CMS/PileupSystematicErrors]] for a more
complete description).

The MC pileup file is created by running the following ROOT macro:

#+BEGIN_SRC sh
    root -l scripts/createPileUpMC.C
#+END_SRC

The macro contains the entries of the histogram to be filled. These values can
be found...

The data pileup files are created by executing the following in the relevant
CMSSW release:

#+BEGIN_SRC sh
    pileupCalc.py -i MyAnalysisJSON.txt --inputLumiJSON pileup_latest.txt --
    calcMode true --minBiasXsec 69000 --maxPileupBin 50 --numPileupBins 50
    MyDataPileupHistogram.root
#+END_SRC

Where =MyAnalysisJSON.txt= is the JSON used in creating the data nTuples,
=pileup_latest.txt= is the latest pileup JSON For scale up/down, just vary the
inelastic cross-section by the prescribed uncertainty (currently +/- 2.7%).

For further info, see:

[[https://twiki.cern.ch/twiki/bin/view/CMS/PileupJSONFileforData#2015_Pileup_JSON_Files]]

The latest JSON file for pileup can be found here:

#+BEGIN_EXAMPLE
    afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions15/13TeV/PileUp/pileup_latest.txt
#+END_EXAMPLE

** Tagging Systematics

The b-tagging systematics require b-tagging efficiency plots to be
created. These b-tag efficiency plots are made for the MC samples in the
=Cuts::makeJetCuts= function in =src/common/cutClass.cpp=.

Using these plots, the function =Cuts::getBWeight= in the same file
reads out these efficiencies and loads the scale factors from
comma-separated-value files using the =BTagCalibration= class provided
by CMS. The current csv files can be found here:

[[https://twiki.cern.ch/twiki/bin/viewauth/CMS/BtagRecommendation76X]]

** PDF Systematics:

The PDF systematics are applied between lines 944-983 in
=src/common/analysisAlgo.cpp= and the function used is initialised on
line 584 of the same file. It is intended to separate these parts of the
code from the main body of code into a separate function in the near
future. The current PDF set to be used can be found here:

[[https://twiki.cern.ch/twiki/bin/view/CMS/TopSystematics#PDF_uncertainties]]

** Factorisation and Renormalisation Scales and Matching Scale Systematics

In the past dedicated MC samples with factorisation and renormalisation
coherently varied scales for the Matrix Element (ME) and Parton Shower
(PS) step of the generator were required. For Run 2 we the per-event
weights in the generator are available at miniAOD level and can be used
to reweight the event for the factorisation and renormalisation effects.
Whilst these weights are saved in the nTuples, their use is not yet
implemented in the analysis code. This is intended to be done when more
progress at the earlier stages of the analysis have made more progress.

It is worth noting that tW samples do not have these event weights, as
scale variations are not possible via LHE weights in Powerheg V1 and
this process is not available in Powerheg V2 yet.

* Potential Problems

Users are recommended not to run Crab3 setup scripts before using this
program. It has been found that this can cause compilation issues when
using the makefile and also causes problems when using the python
scripts as it apparently unloads NumPy.
